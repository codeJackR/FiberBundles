{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dipy.io.streamline import load_trk\n",
    "from dipy.viz import window, actor\n",
    "import math\n",
    "from nibabel import trackvis as tv\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.io.pickles import save_pickle\n",
    "from dipy.data import get_data\n",
    "from dipy.segment.metric import mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_streamlines(streamlines): #function to visualize streamlines\n",
    "    \n",
    "    ren = window.Renderer()\n",
    "    ren.add(actor.line(streamlines))\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_multiple_streamlines(streamline1,streamline2): #function to visualize streamlines\n",
    "    \n",
    "    ren = window.Renderer()\n",
    "    ren.add(actor.line(streamline1))\n",
    "    ren.add(actor.line(streamline2,colors=(0,1,1)))\n",
    "    window.show(ren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " load brain (tractogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain, b_hr = load_trk(\"brain.trk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load Arcuate Fasciculus left fiber bundle (AF_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle, bn_hr = load_trk(\"AF_L.trk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cst, bn_hr = load_trk(\"CST_recognized.trk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first brain\n",
    "\n",
    "#show_streamlines(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rp7/anaconda3/lib/python3.6/site-packages/vtk/util/numpy_support.py:137: FutureWarning: Conversion of the second argument of issubdtype from `complex` to `np.complexfloating` is deprecated. In future, it will be treated as `np.complex128 == np.dtype(complex).type`.\n",
      "  assert not numpy.issubdtype(z.dtype, complex), \\\n"
     ]
    }
   ],
   "source": [
    "# now bundle\n",
    "\n",
    "show_streamlines(bundle[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bundle[0].shape)\n",
    "len(bundle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "\n",
    "def getLabels(brain, bundle):\n",
    "    labels = np.zeros(len(brain))\n",
    "    npbrain = np.array(brain)\n",
    "    for i in range(len(bundle)):\n",
    "        for j in range(len(brain)):\n",
    "            if((npbrain[j].shape[0]==bundle[i].shape[0])):\n",
    "    #         print(i,j)\n",
    "                if np.allclose(npbrain[j], bundle[i]):\n",
    "                    labels[j] = 1\n",
    "                    break\n",
    "        if(i%100==0):\n",
    "            print(i,end=\" \")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-39ef5e8b2288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-9db3670a472a>\u001b[0m in \u001b[0;36mgetLabels\u001b[0;34m(brain, bundle)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpbrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbundle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#         print(i,j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpbrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nibabel/streamlines/array_sequence.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \"\"\"\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = np.zeros(len(brain))\n",
    "l = getLabels(brain, bundle)\n",
    "labels[l==l] = 1\n",
    "l = getLabels(brain, cst)\n",
    "labels[l==l] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Frechet distance\n",
    "# From : https://gist.github.com/MaxBareiss/ba2f9441d9455b56fbc9\n",
    "\n",
    "# Euclidean distance.\n",
    "def euc_dist(pt1,pt2):\n",
    "    return math.sqrt((pt2[0]-pt1[0])*(pt2[0]-pt1[0])+(pt2[1]-pt1[1])*(pt2[1]-pt1[1]))\n",
    "\n",
    "def _c(ca,i,j,P,Q):\n",
    "    if ca[i,j] > -1:\n",
    "        return ca[i,j]\n",
    "    elif i == 0 and j == 0:\n",
    "        ca[i,j] = euc_dist(P[0],Q[0])\n",
    "    elif i > 0 and j == 0:\n",
    "        ca[i,j] = max(_c(ca,i-1,0,P,Q),euc_dist(P[i],Q[0]))\n",
    "    elif i == 0 and j > 0:\n",
    "        ca[i,j] = max(_c(ca,0,j-1,P,Q),euc_dist(P[0],Q[j]))\n",
    "    elif i > 0 and j > 0:\n",
    "        ca[i,j] = max(min(_c(ca,i-1,j,P,Q),_c(ca,i-1,j-1,P,Q),_c(ca,i,j-1,P,Q)),euc_dist(P[i],Q[j]))\n",
    "    else:\n",
    "        ca[i,j] = float(\"inf\")\n",
    "    return ca[i,j]\n",
    "\n",
    "\"\"\" Computes the discrete frechet distance between two polygonal lines\n",
    "Algorithm: http://www.kr.tuwien.ac.at/staff/eiter/et-archive/cdtr9464.pdf\n",
    "P and Q are arrays of 2-element arrays (points)\n",
    "\"\"\"\n",
    "def frechetDist(P,Q):\n",
    "    ca = np.ones((len(P),len(Q)))\n",
    "    ca = np.multiply(ca,-1)\n",
    "    return _c(ca,len(P)-1,len(Q)-1,P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Dynamic time warping\n",
    "# From: https://github.com/slaypni/fastdtw\n",
    "\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.align.streamlinear import BundleMinDistanceMetric\n",
    "from dipy.align.streamlinear import set_number_of_points\n",
    "# BundleMinDistanceMetric().distance(cBrain3D[0],cBrain3D[1])\n",
    "\n",
    "def bmd(a,b):\n",
    "    static = set_number_of_points(a, len(a))\n",
    "    moving = set_number_of_points(b, len(b))\n",
    "    BMD = BundleMinDistanceMetric()\n",
    "    BMD.setup(static, moving)\n",
    "    x0 = np.array([0, 0, 0, 0, 0, 0, 1., 1., 1, 0, 0, 0])  # affine\n",
    "    bmd_value = BMD.distance(x0.tolist())\n",
    "    return bmd_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,3) and (2091,) not aligned: 3 (dim 1) != 2091 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-01222e27763a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain3D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain3D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# brain3D[1].shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-c5d903abd4b2>\u001b[0m in \u001b[0;36mbmd\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mBMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# affine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbmd_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBMD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmd_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FiberBundles/dipy/dipy/align/streamlinear.py\u001b[0m in \u001b[0;36mdistance\u001b[0;34m(self, xopt)\u001b[0m\n\u001b[1;32m    115\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_centered_pts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                                         self.num_threads)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/FiberBundles/dipy/dipy/align/streamlinear.py\u001b[0m in \u001b[0;36mbundle_min_distance_fast\u001b[0;34m(t, static, moving, block_size, num_threads)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0maff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompose_matrix44\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0mmoving\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m     \u001b[0mmoving\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,3) and (2091,) not aligned: 3 (dim 1) != 2091 (dim 0)"
     ]
    }
   ],
   "source": [
    "bmd(brain3D[0],brain3D[7])\n",
    "# brain3D[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance between 2 curves\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def distanceBetweenCurves(c1, c2, algo='fastdtw'):\n",
    "    if algo=='fastdtw':\n",
    "        distance, path = fastdtw(c1,c2, radius = 1, dist = euclidean) # check out the 'radius' parameter to tune the distance\n",
    "        return distance\n",
    "    elif algo=='frechet':\n",
    "        return frechetDist(c1,c2)\n",
    "    elif algo=='bmd':\n",
    "        return bmd(c1,c2)\n",
    "    elif algo=='mdf':\n",
    "        return mdf(c1,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def project2D(streamline):\n",
    "    pca = PCA(n_components=2)\n",
    "    newBundle = pca.fit_transform(streamline)\n",
    "    newStreamline = Streamlines([(np.hstack((newBundle,np.zeros((newBundle.shape[0])).reshape((-1,1)))))])\n",
    "    return newStreamline, pca.explained_variance_ratio_, pca.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pca.explained_variance_ratio_) \n",
    "# print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = Streamlines([newBundle])\n",
    "def convertToAngles(streamline2D):\n",
    "    streamline2D = np.array(streamline2D)\n",
    "#     print((streamline2D))\n",
    "    angles = np.zeros(streamline2D.shape[0])\n",
    "    angles[0] = 1\n",
    "#     print(streamline2D.shape)\n",
    "    # handle the direction of start\n",
    "#     print(streamline2D.shape)\n",
    "    for point in range(1,streamline2D.shape[0]):\n",
    "#         print(point)\n",
    "        diff = streamline2D[point] - streamline2D[point-1]\n",
    "#         print(streamline2D[point], streamline2D[point-1])\n",
    "        if(diff[0]==0):\n",
    "            angles[point] = angles[point-1]\n",
    "        else:\n",
    "            angles[point] = math.degrees(math.atan(diff[1]/diff[0]))\n",
    "    return angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLD(bundle):\n",
    "    data2D = []\n",
    "    angularData = []\n",
    "    for index in range(len(bundle)):\n",
    "        newStreamline, varRatio, singVal = project2D(np.array(bundle[index]))\n",
    "        data2D.append(newStreamline[0][:,:2])\n",
    "#         print(np.array(bundle[index]))\n",
    "        # append these values to store them \n",
    "#         print(len(newStreamline))\n",
    "        streamlineAsAngle = convertToAngles(newStreamline[0])\n",
    "#         print(streamlineAsAngle)\n",
    "        angularData.append(streamlineAsAngle)\n",
    "    \n",
    "    return angularData, data2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bundle1D, bundle2D = getDataLD(bundle[1950:])\n",
    "len(bundle1D)\n",
    "brain1D, brain2D = getDataLD(brain[1950:2150])\n",
    "brain3D = np.array(brain[1950:2150]).tolist()\n",
    "# show_multiple_streamlines(newStreamline,bundle[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "labels = np.ones(len(brain1D))\n",
    "labels[50:]*=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center the data\n",
    "def centerData(data):\n",
    "    for i in range(len(data)):\n",
    "        mean = np.mean(data[i],axis=0)\n",
    "        data[i] = data[i]-mean\n",
    "    return data\n",
    "\n",
    "cBrain1D = centerData(brain1D)\n",
    "cBrain2D = centerData(brain2D)\n",
    "cBrain3D = centerData(brain3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.766499224626674"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distanceBetweenCurves(cBrain3D[3], cBrain3D[40], 'mdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create covariance matrix\n",
    "\n",
    "# cov = np.zeros((len(cBrain3D), len(cBrain3D)))\n",
    "# for i in range(len(cBrain3D)):\n",
    "#     for j in range(len(cBrain3D)):\n",
    "#         cov[i][j] = distanceBetweenCurves(cBrain3D[i], cBrain3D[j], 'frechet')\n",
    "#     print(i, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the covariance matrix variable to disk\n",
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open('covarianceMDF3D.pkl', 'wb') as f:\n",
    "    pickle.dump(cov, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "cov = []\n",
    "with open('covariance3D.pkl', 'rb') as f:\n",
    "    cov = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb223c3320>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "plt.imshow(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "# get centroids for the bundles\n",
    "\n",
    "%time\n",
    "def getCentroids(streamlines, noOfCentroids):\n",
    "    thresh=1\n",
    "    qb = QuickBundles(threshold=thresh)\n",
    "    clusters = qb.cluster(streamlines)\n",
    "    length = len(clusters.centroids)\n",
    "    prev = 0\n",
    "    flag=False\n",
    "    while(length!=noOfCentroids):\n",
    "        if(length>noOfCentroids):\n",
    "            if flag:\n",
    "                temp = thresh\n",
    "                thresh= thresh + (thresh-prev)/2.0\n",
    "                prev = temp\n",
    "            else:\n",
    "                prev=thresh\n",
    "                thresh *= 2.0\n",
    "        else:\n",
    "            temp = thresh\n",
    "            thresh = prev+(thresh-prev)/2.0\n",
    "            flag = True\n",
    "        qb = QuickBundles(threshold=thresh)\n",
    "        clusters = qb.cluster(streamlines)\n",
    "        length = len(clusters)\n",
    "    return np.array(clusters.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def getAllCentroids(brainData, labels, noOfBundles, noOfCentroids=3):\n",
    "    centroids = []\n",
    "    for n in range(noOfBundles):\n",
    "        if n==0:\n",
    "            c = getCentroids(brainData[labels==n], noOfCentroids)\n",
    "            centroids = c.reshape((1,)+c.shape)\n",
    "        else:\n",
    "            centroids = np.vstack((centroids, getCentroids(brainData[labels==n], noOfCentroids).reshape((1,)+c.shape)))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape = (no of bundles, no of centroids in each bundle, no of points in each centroid, dimensions of the points)\n",
    "# centroidMat = getAllCentroids(np.array(cBrain3D),labels, len(np.unique(labels)))\n",
    "\n",
    "# Save\n",
    "# with open('centroidMat.pkl', 'wb') as f:\n",
    "#     pickle.dump(centroidMat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "cov = []\n",
    "with open('centroidMat.pkl', 'rb') as f:\n",
    "    centroidMat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 12, 3)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroidMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distance of a streamline from all the centroids\n",
    "def getDistToCentroids(streamline):\n",
    "    global centroidMat\n",
    "    distMat = np.zeros((centroidMat.shape[0], centroidMat.shape[1]))\n",
    "    for i in range(distMat.shape[0]):\n",
    "        for j in range(distMat.shape[1]):\n",
    "            distMat[i][j] = distanceBetweenCurves(centroidMat[i][j], streamline, 'mdf')\n",
    "    return distMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probability of getting a bundle given a streamline(shape)\n",
    "def pB_S(streamline, metric='min'):\n",
    "    distMat = getDistToCentroids(streamline)\n",
    "    if metric=='min':\n",
    "        distToBundles = np.min(distMat, axis=1)\n",
    "    elif metric=='avg':\n",
    "        distToBundles = np.mean(distMat, axis=1)\n",
    "    \n",
    "    probabilities = 1/distToBundles\n",
    "    probabilities /= np.sum(probabilities)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53049917, 0.46950083])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pB_S(cBrain3D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_streamlines(cst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_multiple_streamlines(bundle,cst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see its data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bundle # whole bundle, made of several streamlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((brain)[0]).shape# number of streamlines in bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle[1] #one streamline in bundle\n",
    "# (np.array(bundle)[0]).shape\n",
    "# np.array(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (bundle[1].shape) #one point on one streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "npbrain = np.array(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npbrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are x, y and z co-ordinates of the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 "
     ]
    }
   ],
   "source": [
    "# get labels\n",
    "\n",
    "def getLabels(brain, bundle):\n",
    "    labels = np.zeros(len(brain))\n",
    "    npbrain = np.array(brain)\n",
    "    for i in range(len(bundle)):\n",
    "        for j in range(len(brain)):\n",
    "            if((npbrain[j].shape[0]==bundle[i].shape[0])):\n",
    "    #         print(i,j)\n",
    "                if np.allclose(npbrain[j], bundle[i]):\n",
    "                    labels[j] = 1\n",
    "                    break\n",
    "        if(i%100==0):\n",
    "            print(i,end=\" \")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npbrain[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts = [len(streamline) for streamline in bundle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.tracking.streamline import (set_number_of_points, nbytes,select_random_set_of_streamlines)\n",
    "newStreamlines = set_number_of_points(bundle,101)\n",
    "n_pts_ds = [len(streamline) for streamline in newStreamlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newBrain = set_number_of_points(brain,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newBrain[0].shape\n",
    "trainset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(npbrain)\n",
    "npbrain = np.array(brain1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.02\n",
    "training_steps = 70\n",
    "batch_size = 100\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = 1000 # Sequence max length\n",
    "n_hidden = 64 # hidden layer num of features\n",
    "n_classes = 2 # linear sequence or not\n",
    "\n",
    "trainset = npbrain[1950:2050]\n",
    "testset = labels[1950:2050]\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 3])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Placeholder', 'Placeholder_1', 'Placeholder_2', 'random_normal/shape', 'random_normal/mean', 'random_normal/stddev', 'random_normal/RandomStandardNormal', 'random_normal/mul', 'random_normal', 'Variable', 'Variable/Assign', 'Variable/read', 'random_normal_1/shape', 'random_normal_1/mean', 'random_normal_1/stddev', 'random_normal_1/RandomStandardNormal', 'random_normal_1/mul', 'random_normal_1', 'Variable_1', 'Variable_1/Assign', 'Variable_1/read', 'IsVariableInitialized', 'IsVariableInitialized_1', 'init']\n",
      "WARNING:tensorflow:From <ipython-input-122-1df78fe636d0>:10: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-123-3da1ac813a18>:12: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rp7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf_session = keras.backend.get_session()\n",
    "tf_graph = tf.get_default_graph()\n",
    "# tf.reset_default_graph()\n",
    "print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "# tf.contrib.rnn.reset_states()\n",
    "# with tf_session.as_default():\n",
    "#      with tf_graph.as_default():\n",
    "#             tf.reset_default_graph() \n",
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-4f01eda86ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n\u001b[0;32m---> 27\u001b[0;31m                                        seqlen: batch_seqlen})\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Calculate batch accuracy & loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "#         batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        rangelow = ((step-1)*batch_size)%(trainset.shape[0])\n",
    "        rangehigh = ((step)*batch_size)%(trainset.shape[0])\n",
    "        if(rangehigh==rangelow):\n",
    "            rangehigh=rangehigh-1\n",
    "        if(rangehigh<rangelow):\n",
    "            batch_x = np.append(trainset[rangelow:], trainset[:rangehigh])\n",
    "            batch_y = np.append(testset[rangelow:], testset[:rangehigh])\n",
    "        else:\n",
    "            batch_x = trainset[rangelow:rangehigh]\n",
    "            batch_y = testset[rangelow:rangehigh]\n",
    "        \n",
    "        batch_seqlen = []\n",
    "        for sample in trainset:\n",
    "            batch_seqlen.append(sample.shape[0])\n",
    "\n",
    "#         batch_seqlen = batch_seqlen.tolist()\n",
    "        print(type(batch_seqlen))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToySequenceData(object):\n",
    "    \"\"\" Generate sequence of data with dynamic length.\n",
    "    This class generate samples for training:\n",
    "    - Class 0: linear sequences (i.e. [0, 1, 2, 3,...])\n",
    "    - Class 1: random sequences (i.e. [1, 3, 10, 7,...])\n",
    "\n",
    "    NOTICE:\n",
    "    We have to pad each sequence to reach 'max_seq_len' for TensorFlow\n",
    "    consistency (we cannot feed a numpy array with inconsistent\n",
    "    dimensions). The dynamic calculation will then be perform thanks to\n",
    "    'seqlen' attribute that records every actual sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "                 max_value=1000):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        for i in range(n_samples):\n",
    "            # Random sequence length\n",
    "            len = np.random.randint(min_seq_len, max_seq_len)\n",
    "            # Monitor sequence length for TensorFlow dynamic calculation\n",
    "            self.seqlen.append(len)\n",
    "            # Add a random or linear int sequence (50% prob)\n",
    "            if np.random.random() < .5:\n",
    "                # Generate a linear sequence\n",
    "                rand_start = np.random.randint(0, max_value - len)\n",
    "                s = [[float(i)/max_value] for i in\n",
    "                     range(rand_start, rand_start + len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([1., 0.])\n",
    "            else:\n",
    "                # Generate a random sequence\n",
    "                s = [[float(np.random.randint(0, max_value))/max_value]\n",
    "                     for i in range(len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([0., 1.])\n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        print(type(batch_seqlen[0]))\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\n",
    "te = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_x, b_y, b_seqlen = tr.next(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((b_x))\n",
    "# print(batch_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
